{"name":"fairseq-preprocess","description":"build vocabularies and binarize training data.","usage":"fairseq-preprocess [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL] [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE] [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH] [--tensorboard-logdir TENSORBOARD_LOGDIR] [--wandb-project WANDB_PROJECT] [--azureml-logging] [--seed SEED] [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16] [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE] [--fp16-scale-window FP16_SCALE_WINDOW] [--fp16-scale-tolerance FP16_SCALE_TOLERANCE] [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE] [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp] [--amp-batch-retries AMP_BATCH_RETRIES] [--amp-init-scale AMP_INIT_SCALE] [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ] [--all-gather-list-size ALL_GATHER_LIST_SIZE] [--model-parallel-size MODEL_PARALLEL_SIZE] [--quantization-config-path QUANTIZATION_CONFIG_PATH] [--profile] [--reset-logging]\n                   [--suppress-crashes] [--use-plasma-view] [--plasma-path PLASMA_PATH] [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}] [--tokenizer {moses,nltk,space}] [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}] [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}] [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n                   [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}] [--task TASK] [-s SRC] [-t TARGET] [--trainpref FP] [--validpref FP] [--testpref FP] [--align-suffix FP] [--destdir DIR] [--thresholdtgt N] [--thresholdsrc N] [--tgtdict FP] [--srcdict FP] [--nwordstgt N] [--nwordssrc N] [--alignfile ALIGN] [--dataset-impl FORMAT] [--joined-dictionary] [--only-source] [--padding-factor N] [--workers N] [--dict-only]","options":[{"names":["-h","--help"],"argument":"","description":"show this help message and exit"},{"names":["--no-progress-bar"],"argument":"","description":"disable progress bar"},{"names":["--log-interval"],"argument":"LOG_INTERVAL","description":"log progress every N batches (when progress bar is disabled)"},{"names":["--log-format"],"argument":"{json,none,simple,tqdm}","description":"log format to use"},{"names":["--log-file"],"argument":"LOG_FILE","description":"log file to copy metrics to."},{"names":["--aim-repo"],"argument":"AIM_REPO","description":"path to Aim repository"},{"names":["--aim-run-hash"],"argument":"AIM_RUN_HASH","description":"Aim run hash. If skipped, creates or continues run based on save_dir"},{"names":["--tensorboard-logdir"],"argument":"TENSORBOARD_LOGDIR","description":"path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)"},{"names":["--wandb-project"],"argument":"WANDB_PROJECT","description":"Weights and Biases project name to use for logging"},{"names":["--azureml-logging"],"argument":"","description":"Log scalars to AzureML context"},{"names":["--seed"],"argument":"SEED","description":"pseudo random number generator seed"},{"names":["--cpu"],"argument":"","description":"use CPU instead of CUDA"},{"names":["--tpu"],"argument":"","description":"use TPU instead of CUDA"},{"names":["--bf16"],"argument":"","description":"use bfloat16; implies --tpu"},{"names":["--memory-efficient-bf16"],"argument":"","description":"use a memory-efficient version of BF16 training; implies --bf16"},{"names":["--fp16"],"argument":"","description":"use FP16"},{"names":["--memory-efficient-fp16"],"argument":"","description":"use a memory-efficient version of FP16 training; implies --fp16"},{"names":["--fp16-no-flatten-grads"],"argument":"","description":"don't flatten FP16 grads tensor"},{"names":["--fp16-init-scale"],"argument":"FP16_INIT_SCALE","description":"default FP16 loss scale"},{"names":["--fp16-scale-window"],"argument":"FP16_SCALE_WINDOW","description":"number of updates before increasing loss scale"},{"names":["--fp16-scale-tolerance"],"argument":"FP16_SCALE_TOLERANCE","description":"pct of updates that can overflow before decreasing the loss scale"},{"names":["--on-cpu-convert-precision"],"argument":"","description":"if set, the floating point conversion to fp16/bf16 runs on CPU. This reduces bus transfer time and GPU memory usage."},{"names":["--min-loss-scale"],"argument":"MIN_LOSS_SCALE","description":"minimum FP16/AMP loss scale, after which training is stopped"},{"names":["--threshold-loss-scale"],"argument":"THRESHOLD_LOSS_SCALE","description":"threshold FP16 loss scale from below"},{"names":["--amp"],"argument":"","description":"use automatic mixed precision"},{"names":["--amp-batch-retries"],"argument":"AMP_BATCH_RETRIES","description":"number of retries of same batch after reducing loss scale with AMP"},{"names":["--amp-init-scale"],"argument":"AMP_INIT_SCALE","description":"default AMP loss scale"},{"names":["--amp-scale-window"],"argument":"AMP_SCALE_WINDOW","description":"number of updates before increasing AMP loss scale"},{"names":["--user-dir"],"argument":"USER_DIR","description":"path to a python module containing custom extensions (tasks and/or architectures)"},{"names":["--empty-cache-freq"],"argument":"EMPTY_CACHE_FREQ","description":"how often to clear the PyTorch CUDA cache (0 to disable)"},{"names":["--all-gather-list-size"],"argument":"ALL_GATHER_LIST_SIZE","description":"number of bytes reserved for gathering stats from workers"},{"names":["--model-parallel-size"],"argument":"MODEL_PARALLEL_SIZE","description":"total number of GPUs to parallelize model over"},{"names":["--quantization-config-path"],"argument":"QUANTIZATION_CONFIG_PATH","description":"path to quantization config file"},{"names":["--profile"],"argument":"","description":"enable autograd profiler emit_nvtx"},{"names":["--reset-logging"],"argument":"","description":"when using Hydra, reset the logging at the beginning of training"},{"names":["--suppress-crashes"],"argument":"","description":"suppress crashes when training with the hydra_train entry point so that the main method can return a value (useful for sweeps)"},{"names":["--use-plasma-view"],"argument":"","description":"Store indices and sizes in shared memory"},{"names":["--plasma-path"],"argument":"PLASMA_PATH","description":"path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail."},{"names":["--criterion","--tokenizer","--bpe","--optimizer","--lr-scheduler","--scoring","--task"],"argument":"{adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,label_smoothed_cross_entropy_with_rdrop,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_unit_2pass,speech_to_spectrogram,speech_to_spectrogram_2pass,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}","description":"task"},{"names":["--dataset-impl"],"argument":"FORMAT","description":"output dataset implementation"},{"names":["-s","--source-lang"],"argument":"SRC","description":"source language"},{"names":["-t","--target-lang"],"argument":"TARGET","description":"target language"},{"names":["--trainpref"],"argument":"FP","description":"train file prefix (also used to build dictionaries)"},{"names":["--validpref"],"argument":"FP","description":"comma separated, valid file prefixes (words missing from train set are replaced with <unk>)"},{"names":["--testpref"],"argument":"FP","description":"comma separated, test file prefixes (words missing from train set are replaced with <unk>)"},{"names":["--align-suffix"],"argument":"FP","description":"alignment file suffix"},{"names":["--destdir"],"argument":"DIR","description":"destination dir"},{"names":["--thresholdtgt"],"argument":"N","description":"map words appearing less than threshold times to unknown"},{"names":["--thresholdsrc"],"argument":"N","description":"map words appearing less than threshold times to unknown"},{"names":["--tgtdict"],"argument":"FP","description":"reuse given target dictionary"},{"names":["--srcdict"],"argument":"FP","description":"reuse given source dictionary"},{"names":["--nwordstgt"],"argument":"N","description":"number of target words to retain"},{"names":["--nwordssrc"],"argument":"N","description":"number of source words to retain"},{"names":["--alignfile"],"argument":"ALIGN","description":"an alignment file (optional)"},{"names":["--joined-dictionary"],"argument":"","description":"Generate joined dictionary"},{"names":["--only-source"],"argument":"","description":"Only process the source language"},{"names":["--padding-factor"],"argument":"N","description":"Pad dictionary size to be multiple of N"},{"names":["--workers"],"argument":"N","description":"number of parallel workers"},{"names":["--dict-only"],"argument":"","description":"if true, only builds a dictionary and then exits"}],"version":"0.12.2"}
