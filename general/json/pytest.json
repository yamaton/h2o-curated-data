{"name":"pytest","description":"pytest","options":[{"names":["-k"],"argument":"EXPRESSION","description":"only run tests which match the given substring expression. An expression is a python evaluatable expression where all names are substring-matched against test names and their parent classes. Example: -k 'test_method or test_other' matches all test functions and classes whose name contains 'test_method' or 'test_other', while -k 'not test_method' matches those that don't contain 'test_method' in their names. -k 'not test_method and not test_other' will eliminate the matches. Additionally keywords are matched to classes and functions containing extra names in their 'extra_keyword_matches' set, as well as functions which have names assigned directly to them. The matching is case-insensitive."},{"names":["-m"],"argument":"MARKEXPR","description":"only run tests matching given mark expression.   For example: -m 'mark1 and not mark2'."},{"names":["--markers"],"argument":"","description":"show markers (builtin, plugin and per-project ones)."},{"names":["-x","--exitfirst"],"argument":"","description":"exit instantly on first error or failed test."},{"names":["--fixtures","--funcargs"],"argument":"","description":"show available fixtures, sorted by plugin appearance (fixtures with leading '_' are only shown with '-v')"},{"names":["--fixtures-per-test"],"argument":"","description":"show fixtures per test"},{"names":["--pdb"],"argument":"","description":"start the interactive Python debugger on errors or KeyboardInterrupt."},{"names":["--pdbcls"],"argument":"modulename:classname","description":"specify a custom interactive Python debugger for use with --pdb.For example: --pdbcls=IPython.terminal.debugger:TerminalPdb"},{"names":["--trace"],"argument":"","description":"Immediately break when running each test."},{"names":["--capture"],"argument":"method","description":"per-test capturing method: one of fd|sys|no|tee-sys."},{"names":["-s"],"argument":"","description":"shortcut for --capture=no."},{"names":["--runxfail"],"argument":"","description":"report the results of xfail tests as if they were not marked"},{"names":["--lf","--last-failed"],"argument":"","description":"rerun only the tests that failed at the last run (or all if none failed)"},{"names":["--ff","--failed-first"],"argument":"","description":"run all tests, but run the last failures first.   This may re-order tests and thus lead to repeated fixture setup/teardown."},{"names":["--nf","--new-first"],"argument":"","description":"run tests from new files first, then the rest of the tests sorted by file mtime"},{"names":["--cache-show"],"argument":"[CACHESHOW]","description":"show cache contents, don't perform collection or tests. Optional argument: glob (default: '*')."},{"names":["--cache-clear"],"argument":"","description":"remove all cache contents at start of test run."},{"names":["--lfnf","--last-failed-no-failures"],"argument":"{all,none}","description":"which tests to run with no previously (known) failures."},{"names":["--sw","--stepwise"],"argument":"","description":"exit on test failure and continue from last failing test next time"},{"names":["--sw-skip","--stepwise-skip"],"argument":"","description":"ignore the first failing test but stop on the next failing test. implicitly enables --stepwise."},{"names":["--durations"],"argument":"N","description":"show N slowest setup/test durations (N=0 for all)."},{"names":["--durations-min"],"argument":"N","description":"Minimal duration in seconds for inclusion in slowest list. Default 0.005"},{"names":["-v","--verbose"],"argument":"","description":"increase verbosity."},{"names":["--no-header"],"argument":"","description":"disable header"},{"names":["--no-summary"],"argument":"","description":"disable summary"},{"names":["-q","--quiet"],"argument":"","description":"decrease verbosity."},{"names":["--verbosity"],"argument":"VERBOSE","description":"set verbosity. Default is 0."},{"names":["-r"],"argument":"chars","description":"show extra test summary info as specified by chars: (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. (w)arnings are enabled by default (see --disable-warnings), 'N' can be used to reset the list. (default: 'fE')."},{"names":["--disable-warnings","--disable-pytest-warnings"],"argument":"","description":"disable warnings summary"},{"names":["-l","--showlocals"],"argument":"","description":"show locals in tracebacks (disabled by default)."},{"names":["--tb"],"argument":"style","description":"traceback print mode (auto/long/short/line/native/no)."},{"names":["--show-capture"],"argument":"{no,stdout,stderr,log,all}","description":"Controls how captured stdout/stderr/log is shown on failed tests. Default is 'all'."},{"names":["--full-trace"],"argument":"","description":"don't cut any tracebacks (default is to cut)."},{"names":["--color"],"argument":"color","description":"color terminal output (yes/no/auto)."},{"names":["--code-highlight"],"argument":"{yes,no}","description":"Whether code should be highlighted (only if --color is also enabled)"},{"names":["--pastebin"],"argument":"mode","description":"send failed|all info to bpaste.net pastebin service."},{"names":["--junit-xml"],"argument":"path","description":"create junit-xml style report file at given path."},{"names":["--junit-prefix"],"argument":"str","description":"prepend prefix to classnames in junit-xml output"},{"names":["-W","--pythonwarnings"],"argument":"PYTHONWARNINGS","description":"set which warnings to report, see -W option of python itself."},{"names":["--maxfail"],"argument":"num","description":"exit after first num failures or errors."},{"names":["--strict-config"],"argument":"","description":"any warnings encountered while parsing the `pytest` section of the configuration file raise errors."},{"names":["--strict-markers"],"argument":"","description":"markers not registered in the `markers` section of the configuration file raise errors."},{"names":["--strict"],"argument":"","description":"(deprecated) alias to --strict-markers."},{"names":["-c"],"argument":"file","description":"load configuration from `file` instead of trying to locate one of the implicit configuration files."},{"names":["--continue-on-collection-errors"],"argument":"","description":"Force test execution even if collection errors occur."},{"names":["--rootdir"],"argument":"ROOTDIR","description":"Define root directory for tests. Can be relative path: 'root_dir', './root_dir', 'root_dir/another_dir/'; absolute path: '/home/user/root_dir'; path with variables: '$HOME/root_dir'."},{"names":["--collect-only","--co"],"argument":"","description":"only collect tests, don't execute them."},{"names":["--pyargs"],"argument":"","description":"try to interpret all arguments as python packages."},{"names":["--ignore"],"argument":"path","description":"ignore path during collection (multi-allowed)."},{"names":["--ignore-glob"],"argument":"path","description":"ignore path pattern during collection (multi-allowed)."},{"names":["--deselect"],"argument":"nodeid_prefix","description":"deselect item (via node id prefix) during collection (multi-allowed)."},{"names":["--confcutdir"],"argument":"dir","description":"only load conftest.py's relative to specified dir."},{"names":["--noconftest"],"argument":"","description":"Don't load any conftest.py files."},{"names":["--keep-duplicates"],"argument":"","description":"Keep duplicate tests."},{"names":["--collect-in-virtualenv"],"argument":"","description":"Don't ignore tests in a local virtualenv directory"},{"names":["--import-mode"],"argument":"{prepend,append,importlib}","description":"prepend/append to sys.path when importing test modules and conftest files, default is to prepend."},{"names":["--doctest-modules"],"argument":"","description":"run doctests in all .py modules"},{"names":["--doctest-report"],"argument":"{none,cdiff,ndiff,udiff,only_first_failure}","description":"choose another output format for diffs on doctest failure"},{"names":["--doctest-glob"],"argument":"pat","description":"doctests file matching pattern, default: test*.txt"},{"names":["--doctest-ignore-import-errors"],"argument":"","description":"ignore doctest ImportErrors"},{"names":["--doctest-continue-on-failure"],"argument":"","description":"for a given doctest, continue to run after the first failure"},{"names":["--basetemp"],"argument":"dir","description":"base temporary directory for this test run.(warning: this directory is removed if it exists)"},{"names":["-V","--version"],"argument":"","description":"display pytest version and information about plugins. When given twice, also display information about plugins."},{"names":["-h","--help"],"argument":"","description":"show help message and configuration info"},{"names":["-p"],"argument":"name","description":"early-load given plugin module name or entry point (multi-allowed).   To avoid loading of plugins, use the `no:` prefix, e.g. `no:doctest`."},{"names":["--trace-config"],"argument":"","description":"trace considerations of conftest.py files."},{"names":["--debug"],"argument":"[DEBUG_FILE_NAME]","description":"store internal tracing debug information in this log file. This file is opened with 'w' and truncated as a result, care advised. Defaults to 'pytestdebug.log'."},{"names":["-o","--override-ini"],"argument":"OVERRIDE_INI","description":"override ini option with \"option=value\" style, e.g. `-o xfail_strict=True -o cache_dir=cache`."},{"names":["--assert"],"argument":"MODE","description":"Control assertion debugging tools.   'plain' performs no assertion debugging. 'rewrite' (the default) rewrites assert statements in test modules on import to provide assert expression information."},{"names":["--setup-only"],"argument":"","description":"only setup fixtures, do not execute tests."},{"names":["--setup-show"],"argument":"","description":"show setup of fixtures while executing tests."},{"names":["--setup-plan"],"argument":"","description":"show what fixtures and tests would be executed but don't execute anything."},{"names":["--log-level"],"argument":"LEVEL","description":"level of messages to catch/display.   Not set by default, so it depends on the root/parent log handler's effective level, where it is \"WARNING\" by default."},{"names":["--log-format"],"argument":"LOG_FORMAT","description":"log format as used by the logging module."},{"names":["--log-date-format"],"argument":"LOG_DATE_FORMAT","description":"log date format as used by the logging module."},{"names":["--log-cli-level"],"argument":"LOG_CLI_LEVEL","description":"cli logging level."},{"names":["--log-cli-format"],"argument":"LOG_CLI_FORMAT","description":"log format as used by the logging module."},{"names":["--log-cli-date-format"],"argument":"LOG_CLI_DATE_FORMAT","description":"log date format as used by the logging module."},{"names":["--log-file"],"argument":"LOG_FILE","description":"path to a file when logging will be written to."},{"names":["--log-file-level"],"argument":"LOG_FILE_LEVEL","description":"log file logging level."},{"names":["--log-file-format"],"argument":"LOG_FILE_FORMAT","description":"log format as used by the logging module."},{"names":["--log-file-date-format"],"argument":"LOG_FILE_DATE_FORMAT","description":"log date format as used by the logging module."},{"names":["--log-auto-indent"],"argument":"LOG_AUTO_INDENT","description":"Auto-indent multiline messages passed to the logging module. Accepts true|on, false|off or an integer."}],"version":"pytest 7.1.2","tldr":"> Run Python tests.\n> More information: <https://docs.pytest.org/>.\n\n- Run tests from specific files:\n\n`pytest {{path/to/test_file1.py path/to/test_file2.py ...}}`\n\n- Run tests with names matching a specific [k]eyword expression:\n\n`pytest -k {{expression}}`\n\n- Exit as soon as a test fails or encounters an error:\n\n`pytest --exitfirst`\n\n- Run tests matching or excluding markers:\n\n`pytest -m {{marker_name1 and not marker_name2}}`\n\n- Run until a test failure, continuing from the last failing test:\n\n`pytest --stepwise`\n\n- Run tests without capturing output:\n\n`pytest --capture=no`\n"}
